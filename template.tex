\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage[ruled,vlined]{algorithm2e}

\title{A reinforcement-learning interface environment to design and evaluate adversarial attacks}


\author{
  Gilad Ben Or\\
  ID: 036256527\\
  \texttt{benorgi@post.bgu.ac.il} \\
  }


\begin{document}
\maketitle

\begin{abstract}
Adversarial examples are maliciously manipulated examples that fool machine learning models. Decision-based (D.B.) attacks are a form of black box attacks that rely solely on the final decision of the model. In this paper, I am presenting a framework based on  GYM environment that enables researchers to plan and test their D.B. attacks. This framework, together with CleverHans environment, may improve research cycles and open the mind to new algorithms, in particular reinforcement learning algorithm.
In addition, I propose a novel deterministic algorithm and a reinforcement learning algorithm to fool MNIST classifier and test them both using this framework.
\end{abstract}

% keywords can be removed
\keywords{reinforcement-learning \and adversarial \and gym \and boundary}

% --------------------
\section{Introduction}
% --------------------
Many high-performance machine learning algorithms are susceptible to minimal changes in their inputs. Adversarial perturbations drawn attention from two directions. On the one hand, we are concerned about the integrity and security of machine learning algorithms deployed, such as autonomous cars or face-recognition systems. On the other hand, it is a exciting research opportunity to understand the difference between human and computer sensory information processing.

Decision-based attacks are a category of black box attacks which depend solely on the model's final decision. In terms of the structure of the inputs and outputs and certain examples classified by the model, the attacker has little knowledge regarding the  the model to be attacked.

Previous works suggested algorithms for finding adversarial examples with minimal perturbation size and minimal calls of the model to be attacked. \cite{brendel2017decision} introduced a the boundary attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. \cite{chen2019hopskipjumpattack} proposed a novel unbiased estimate of gradient direction at the decision boundary based solely on access to model decisions. Their algorithms requires significantly fewer model queries than several state-of-the-art decision-based adversarial attacks and achieves competitive performance in attacking several widely-used defense mechanisms.

In order to design and evaluate more attack algorithms, there is a real need for an environment that allows data interaction and results measurement. An existing, well known and powerful environment is CleverHans. CleverHans \cite{papernot2018cleverhans} is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. 

I looked at the same problems that CleverHans encountered and sought to address in a different way. I have observed that the requirements of the testing environment are similar and may be reduced to those of the reinforcement learning environment.
Reinforcement learning environment offers the opportunity to observe data , perform a single or sequence of actions and receive the corresponding score. Here, the attack algorithms are equivalent to the reinforcement learning policies.

The use of such an environment will improve the implementation of new algorithms and will be a standard tool for testing them. Thus, researchers may be able to work with state-of-the-art algorithms and methodologies, reinforcement learning in particular.

A few works used reinforcement learning tools in order to generate adversarial attacks. \cite{hyrum2017evading} implemented a reinforcement learning agent that was equipped with a set of functionality-preserving operations that  may perform on PE file, and learnt through a series of games played against the anti-malware engine, which
sequence of operations is most likely to result in evasion for a given malware sample

In this work, I developed a decision-based attack environment based on the OpenAI Reinforcement Learning Gym \cite{brockman2016openai} interface. The system fits for the MNIST dataset \cite{mnist10027939599} and can quickly be expanded to support other datasets. I have researched and added support for custom actions, implemented two policies and trained a reinforcement learning agent to discover the optimal policy.

The contributions of this work are as follows:
\begin{itemize}
\item Introduce a research framework for decision-based attacks. 
\item Suggests custom actions for attack algorithms.
\item Design, implementation and evaluation of two boundary attack algorithms.
\item Training and evaluation of reinforcement learning agent in this environment.
\end{itemize}

% --------------------
\section{Research Question}
% --------------------
\subsection{Actions}
Actions are the methods of the Agent that allow it to interact and change the environment, and thus to be transferred between states. I would like to propose a set of actions that are good enough to compare the state-of-the-art approaches for blackbox attacks.

\subsection{Reduced model calls}
Blackbox attacks call models to determine their performance and to asses the next step. I would like to explore whether the proposed approach reduces the number of calls for each example.

\subsection{Reinforcement learning vs deterministic algorithm}
I wonder if the  reinforcement learning agent manages, with the same set of actions, to overcome the performance of a deterministic algorithm.

% --------------------
\section{Hypothesis}
% --------------------
I hypothesize that I would be able to identify a series of actions that produce reasonable performance in terms of  of perturbation size and amount of API calls.\\
I believe that the reinforcement learning agent manages to overcome the performance of a deterministic algorithm because it would distill more information for the observation from the environment. However, its running time may be longer in some orders of magnitude.

% --------------------
\section{Material and methods}
% --------------------

\subsection{Data}
I used the MNIST \cite{mnist10027939599} dataset for this work. The dataset was downloaded using Keras API without any additional prepossessing. 
The MNIST dataset has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field. \\

\subsection{The model to be attacked}
The model I would like to attacked is the convolution neural network used during the course labs. It achieved accuracy performance of 0.975.

\subsection{Evaluation environment}
I chose OpenAI Gym \cite{brockman2016openai} as my evaluation environment. Gym is a toolkit for developing and comparing reinforcement learning algorithms. It makes no assumptions about the structure of the tested agent, and is compatible with any numerical computation library, such as TensorFlow or Theano.
I selected this environment as it enables various algorithms to be fitted and tested. In specific, it is appropriate for both the deterministic method and the reinforcement learning algorithm. \\
In order to register a new environment, I implemented the following 4 methods: constructor, reset, render and step.
\subsubsection{Constructor}
In the environment's constructor I loaded the dataset, initiated the MNIST classifier (the model to be attacked) and checked that it perform well.

\subsubsection{Reset}
This method resets the environment to initial state and return first observation (see below).
Here, it does the following things:
\begin{enumerate}
\item Choose a benign example (from the training or the testing sets, depending on the environment mode).
\item Generate two centers (see below).
\item Reset initial variables such as the initial step size, max perturbation and etc.
\end{enumerate}

\subsubsection{Step}
This method commits an action a and return a new observation,  a reward, the is done signal and additional info (all explained below).

\subsubsection{Observation}
The observation is an environment-specific object representing the agent's observation of the environment. The observation consists of the following objects:
\begin{enumerate}
\item \textbf{current image} - the image of the current step.
\item \textbf{classifier prediction} - the classifier's prediction for the current image.
\item \textbf{perturbation norm} - the perturbation norm of the current image with respect to the source image.
\item \textbf{steps} - the index of the current step.
\end{enumerate}

\subsubsection{Action space}
The following actions are available for the agent:
\begin{enumerate}
\item 
\textbf{Decrease step} - 
Decrease the step size according to the equation below.: \\
step=\begin{Bmatrix}
step size > 0.4 : step size/2\\ 
o.w.: max(0.05, step_size- 0.05)
\end{Bmatrix}
\item 
\textbf{Increase step} - 
Increase the step size according to the equation below.: \\
step=\begin{Bmatrix}
step size > 0.4 : step size/2\\ 
o.w.: max(0.05, step_size- 0.05)
\end{Bmatrix}

\item 
\textbf{Closet cluster} - 
Go one step in the direction of the nearest target cluster.

\item 
\textbf{Farthest cluster} - 
Go one step in the direction of the most distant target cluster.

\item 
\textbf{Original image} - 
Go one step in the direction of the original sample.

\item 
\textbf{New centers} - 
Generate new centers. In offline, the environment randomly generates 20 samples with the target label and select a pair of samples with the largest distance between them. The output of this process is 2 target samples which called centers. The agent calculates the distance to each center at each step. The closest target center called "closest cluster" and farthest one called "farthest center".
\end{enumerate}

\subsubsection{Reward}
The reward has to take into account two factors:
\begin{enumerate}
\item \textbf{The classification of the adversarial example} - whether the classifier labels the examples as the target class. \\
\begin{Bmatrix}
1: x\in targetclass\\ 
0: x\notin targetclass
\end{Bmatrix}
\item \textbf{Perturbation size} - the agent gets negative reward if the perturbation is bigger than maximum perturbation P and positive reward if it under this limit.
\end{enumerate}

A suggestion for the reward function is shown in (Equation \ref{eq1}):
\begin{equation} \label{eq1}
reward = \alpha C + \beta  \left (P-d\left ( o, t \right )  \right )
\end{equation}

\subsubsection{Is done signal}
The is done signal stops the agent while it does the exploration-exploitation tasks. We would stop the agent if it perform more than L steps, or it reached the target class under the perturbation size constraint.

\subsection{Deterministic agents}
In this section I describe two deterministic agents that were evaluated in the this work. The agents' policy was described using a graph. The agents moves from initial state and takes an action based on the current state and observation.


\subsubsection{The triangle agent}
The triangle agent used a policy as described in algorithm \ref{alg:simple}. It used one source point and two target points to explore the boundary and then to move in a triangle track from point to point in order to reduce the perturbation. In each cycle, the agent decreases the step size.

\begin{algorithm}[H] \label{alg:simple}
\SetAlgoLined
\KwResult{Adversarial image}
 initialization\;
 \While{perturbation \geq PERTURBATION SIZE, steps \leq MAX STEPS }{
 go in the direction of the closet center until labeled as target;\\
 decrease step size;\\
 go in the direction of the source sample until labeled as source;\\
 (option) decrease step size;\\
 go in the direction of the farthest center until labeled as target;\\
 (option) decrease step size;\\
 go in the direction of the source sample until labeled as source;\\
 (option) decrease step size;\\
 }
 \caption{Triangle agent policy}
\end{algorithm}

\subsubsection{The alternating triangle agent}
The alternating triangle agent is an enhancement of the triangle agent. It executes the triangle agent for K steps, takes the best observation and used it as the initial step for the next agent. It may keep the current step size or increase it back to the initial step size (algorithm \ref{alg:simple2}).

\begin{algorithm}[H] \label{alg:simple2}
\SetAlgoLined
\KwResult{Adversarial image}
 initialization\;
 \For{n in N}{
 Initiate thee agent (random state=k)\\
 Run the triangle agent for K steps.\\
 Pick the sample with the lowest perturbation size.\\
 (option) Increase the step size.\\
}
 \caption{Alternating triangle agent policy}
\end{algorithm}

\subsection{Reinforcement learning model}
The purpose of reinforcement learning model is to teach an agent to take actions in an environment in order to maximize the cumulative reward. In this work, the agent learns how to take actions in order to generate an adversarial example for a target class while maximize the rewards which is relative to its success in generating an adversarial example with minimal perturbation size.

Figure \ref{fig:rl_model} shows a widely used reinforcement learning framework. The agent accesses the environment, triggers it with an action, collects the reward and observes the new state of the environment.

\begin{figure}[H]
    \centering
        \includegraphics[scale=.8]{reinforcement-learning-fig1-700.jpg}
    \caption{Reinforcement learning framework.}
    \label{fig:rl_model}
\end{figure}


\subsubsection{DQN agent}
I used a DQN agent. The deep Q-network (DQN) algorithm is a model-free, online, off-policy reinforcement learning method. A DQN agent is a value-based reinforcement learning agent that trains a critic to estimate the return or future rewards. DQN is a variant of Q-learning. I choose the DQN agent because I have a positive experience with it, and because it has gained human-level power in several of the Atari games that are close to our task.

\subsubsection{Keras-RL}
I used the DQN implementation of Keras-RL. Keras-RL \cite{plappert2016kerasrl} implements some state-of-the art deep reinforcement learning algorithms in Python and seamlessly integrates with the deep learning library Keras. Furthermore, keras-rl works with OpenAI Gym out of the box. 

\subsection{Metrics}
I used 3 metrics to measure the performance of the algorithm: 

\begin{enumerate}
\item Perturbation size - the perturbation size of the adversarial example. 
\item Number of steps - the number of steps the algorithm performs. This measurement is highly correlated with the API calls requested by the algorithm. 
\item Fitting effort - the effort needed to fit the algorithm into a new task.
\end{enumerate}

\subsection{Hyper-parameters}
Table \ref{tab:hyperparam} describes the hyper-parameters which used to control the environment and the agents' learning process.
Because our learning task is an intensive computing task, we used a single set of parameters and did not perform hyper-parameter optimization..

\begin{table}[h]
\caption{Environment and agents hyper-parameters}
\label{tab:hyperparam}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Hyper parameter}    & \textbf{Value} & \textbf{Description}                                          \\ \hline
MAX\_STEPS                  & 2000           & Maximum number of steps for each episode                      \\ \hline
INITIAL\_STEP\_SIZE         & 2              & The initial step size                                         \\ \hline
MIN\_STEP\_SIZE             & 0.05           & The minimal step size                                         \\ \hline
MAX\_STEP\_SIZE             & 2              & the maximal step size                                         \\ \hline
STEPS\_TO\_IMPROVE          & 3              & the number of steps to continue after reach the goal          \\ \hline
SAMPLES\_FOR\_CALC\_CENTERS & 300            & How many samples to use to calculate the centers              \\ \hline
MAX\_PERTURBATION           & 5              & The maximal size of perturbation which gives posisitve reward \\ \hline
REWARD\_COEF                & 10             & The ratio between perturbation reard and label reward         \\ \hline
DQN\_LR                     & 0.01           & The size of the learning rate                                 \\ \hline
WINDOW\_LENGTH              & 4              & The size of the experience replay window                      \\ \hline
TARGET\_MODEL\_UPDATE       & 500            & The rate of updating the dqn target network                   \\ \hline
\end{tabular}
\end{table}


% --------------------
\section{Results}
% --------------------

\subsection{Simple centers algorithm}
\begin{table}[h]
\caption{Simple centers algorithm. The table explains the details about the minimal perturbations that happened in the episodes. Min perturbations is the minimum perturbations reached in all episodes. Max perturbations is the maximum perturbations over all the episodes' perturbation.}
\label{tab:simple_result}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
\hline
\textbf{}       & \textbf{}     & \multicolumn{4}{c|}{\textbf{perturbation}}                      & \multicolumn{4}{c|}{\textbf{steps}}                             \\ \hline
\textbf{target} & \textbf{nobs} & \textbf{min} & \textbf{max} & \textbf{mean} & \textbf{variance} & \textbf{min} & \textbf{max} & \textbf{mean} & \textbf{variance} \\ \hline
0 & 100  & 3.79     & 7.84     & 5.42      & 0.97          & 2         & 1895      & 194.2      & 168529.84      \\ \hline
1 & 100  & 2.0      & 11.33    & 6.23      & 2.37          & 1         & 1273      & 44.42      & 20352.17       \\ \hline
2 & 100  & 2.0      & 9.01     & 5.35      & 1.89          & 1         & 1994      & 191.41     & 216859.19      \\ \hline
3 & 100  & 3.59     & 10.72    & 5.79      & 1.81          & 2         & 1978      & 188.25     & 190226.86      \\ \hline
4 & 100  & 3.55     & 10.04    & 5.78      & 1.71          & 2         & 1632      & 160.87     & 150310.03      \\ \hline
5 & 100  & 3.07     & 9.6      & 6.2       & 1.84          & 2         & 401       & 19.24      & 2625.3         \\ \hline
6 & 100  & 2.0      & 10.24    & 5.87      & 2.01          & 1         & 1987      & 207.7      & 195911.51      \\ \hline
7 & 100  & 3.54     & 9.42     & 6.06      & 2.14          & 2         & 1979      & 192.28     & 211393.82      \\ \hline
8 & 100  & 2.0      & 10.3     & 5.28      & 1.62          & 1         & 1940      & 192.8      & 192080.91      \\ \hline
9 & 100  & 3.8      & 8.73     & 5.69      & 1.34          & 2         & 1930      & 77.56      & 74499.2        \\ \hline
\end{tabular}
\end{table}


for all targets, there is a source example that is very close to the boundary, thus, one or two steps are enough to produce an adversarial example.

\subsection{Iterative centers algorithm}
\subsubsection{Version1 - continue with minimal step size}
\begin{table}[h]
\caption{Iterative centers algorithm results}
\label{tab:iterative_result}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
\hline
\textbf{}       & \textbf{}     & \multicolumn{4}{c|}{\textbf{perturbation}}                      & \multicolumn{4}{c|}{\textbf{steps}}                             \\ \hline
\textbf{target} & \textbf{nobs} & \textbf{min} & \textbf{max} & \textbf{mean} & \textbf{variance} & \textbf{min} & \textbf{max} & \textbf{mean} & \textbf{variance} \\ \hline
0 & 100  & 3.52     & 6.57     & 5.15      & 0.56          & 2         & 1998      & 1240.96    & 337448.91      \\ \hline
1 & 100  & 3.71     & 9.9      & 5.87      & 1.56          & 2         & 1924      & 840.9      & 413978.66      \\ \hline
2 & 100  & 2.0      & 7.26     & 5.02      & 0.86          & 1         & 1999      & 894.11     & 456457.01      \\ \hline
3 & 100  & 3.73     & 8.01     & 5.18      & 1.1           & 2         & 1993      & 1154.96    & 427066.36      \\ \hline
4 & 100  & 3.25     & 7.96     & 5.24      & 1.07          & 2         & 1994      & 1147.4     & 426411.49      \\ \hline
5 & 100  & 3.02     & 8.61     & 5.75      & 1.47          & 2         & 1810      & 240.7      & 131112.35      \\ \hline
6 & 100  & 2.0      & 8.34     & 5.38      & 0.96          & 1         & 1991      & 1391.65    & 250875.6       \\ \hline
7 & 100  & 3.52     & 8.02     & 5.41      & 1.28          & 2         & 1993      & 877.99     & 397813.36      \\ \hline
8 & 100  & 2.0      & 7.18     & 4.91      & 0.75          & 1         & 1940      & 1086.72    & 437924.24      \\ \hline
9 & 100  & 3.75     & 8.62     & 5.39      & 1.16          & 2         & 1989      & 1097.95    & 451468.78      \\ \hline
\end{tabular}
\end{table}

\subsubsection{Version2 - reset the step size to the initial step size value}
\begin{table}[h]
\caption{Iterative centers algorithm results with increasing the step size}
\label{tab:iterative_result_new_init}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
\hline
\textbf{}       & \textbf{}     & \multicolumn{4}{c|}{\textbf{perturbation}}                      & \multicolumn{4}{c|}{\textbf{steps}}                             \\ \hline
\textbf{target} & \textbf{nobs} & \textbf{min} & \textbf{max} & \textbf{mean} & \textbf{variance} & \textbf{min} & \textbf{max} & \textbf{mean} & \textbf{variance} \\ \hline
0 & 100  & 3.23     & 6.53     & 5.07      & 0.51          & 3         & 1990      & 780.97     & 362951.63      \\ \hline
1 & 100  & 3.8      & 9.16     & 6.06      & 1.36          & 2         & 1827      & 569.13     & 337126.05      \\ \hline
2 & 100  & 3.42     & 9.49     & 4.93      & 0.9           & 2         & 1999      & 625.76     & 340929.54      \\ \hline
3 & 100  & 3.33     & 10.6     & 5.11      & 1.35          & 2         & 1999      & 816.65     & 441403.28      \\ \hline
4 & 100  & 3.33     & 8.78     & 5.21      & 1.21          & 2         & 1999      & 801.98     & 408205.86      \\ \hline
5 & 100  & 3.79     & 9.09     & 5.86      & 1.72          & 2         & 1995      & 180.07     & 102842.85      \\ \hline
6 & 100  & 2.0      & 7.51     & 5.3       & 0.91          & 1         & 1997      & 867.24     & 339526.18      \\ \hline
7 & 100  & 3.02     & 9.55     & 5.43      & 1.67          & 2         & 1827      & 622.48     & 341692.9       \\ \hline
8 & 100  & 3.3      & 7.03     & 4.95      & 0.87          & 2         & 1997      & 555.36     & 345146.5       \\ \hline
9 & 100  & 3.36     & 7.9      & 5.05      & 0.94          & 2         & 1835      & 631.79     & 377226.27      \\ \hline
\end{tabular}
\end{table}


\subsection{Reinforcement learning}

\subsubsection{Fitting effort}
It took 40 minutes to perform 10,000 learning steps on the i7 CPU Linux server.
The full fitting task \textbf{for a single target}, lasted more than five days.

\subsubsection{Result}
todo



\subsection{Summary result}

\begin{table}[h]
\caption{Summary of all 4 methods. The table shows the mean pretrubation size for each label}
\label{tab:summary}
\begin{tabular}{|l|l|l|l|l|}
\hline
  & simple & iterative & iterative with new init & RL \\ \hline
0 & 5.42   & 5.15      & 5.07                    &    \\ \hline
1 & 6.23   & 5.87      & 6.06                    &    \\ \hline
2 & 5.35   & 5.02      & 4.93                    &    \\ \hline
3 & 5.79   & 5.18      & 5.11                    &    \\ \hline
4 & 5.78   & 5.24      & 5.21                    &    \\ \hline
5 & 6.2    & 5.75      & 5.86                    &    \\ \hline
6 & 5.87   & 5.38      & 5.3                     &    \\ \hline
7 & 6.06   & 5.41      & 5.43                    &    \\ \hline
8 & 5.28   & 4.91      & 4.95                    &    \\ \hline
9 & 5.69   & 5.39      & 5.05                    &    \\ \hline
\end{tabular}
\end{table}


% --------------------
\section{Discussion and Conclusions}
% --------------------
Your interpretation of the results and findings


% --------------------
\section{Future work}
% --------------------
The first direction for future work is to extend the action space. Adding actions to the environment would make it easier to support more algorithms that can perform better.

Another interesting research direction would be to give the iterative policy to the reinforcement algorithm as its initial policy and to give it the task of refining it. The combination of the two algorithms may achieve a better result with less computational power.

Finally, I would like to recommend that this framework be published as an open source project for the community. I think this complements the CleverHans environment. The use of both together can enhance the researcher's ability to develop and test new algorithms.


\bibliographystyle{apalike}  
\bibliography{references} 

\end{document}
